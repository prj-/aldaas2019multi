# A Multilevel Schwarz Preconditioner Based on a Hierarchy of Robust Coarse Spaces

> A parallel implementation of a multilevel linear solver, part of the HPDDM library and tested in the FreeFEM language.

<p align="center"><img src="https://github.com/prj-/aldaas2019multi/raw/main/header.png" height="300"></p>

The code available in this repository can reproduce the results from the following [paper](https://epubs.siam.org/doi/abs/10.1137/19M1266964).
```
@article{aldaas2019multi,
    Author = {Al Daas, Hussam and Grigori, Laura and Jolivet, Pierre and Tournier, Pierre-Henri},
    Title = {A Multilevel {Schwarz} Preconditioner Based on a Hierarchy of Robust Coarse Spaces},
    Year = {2021},
    Journal = {SIAM Journal on Scientific Computing},
    Volume = {43},
    Number = {3},
    Pages = {A1907--A1928},
    Url = {https://github.com/prj-/aldaas2019multi}
}
```

## Getting started
### Dependencies
Make sure you have access to a recent [FreeFEM](https://freefem.org/) installation (version 4.3 or above), compiled with the `hpddm` plugin. More details about the HPDDM options used in the solver may be found in the [cheat sheet](https://github.com/hpddm/hpddm/blob/main/doc/cheatsheet.pdf).
### Usage example
One should be able to launch the following commands, which solves the scalar diffusion or linear elasticity equations on the same geometrical configurations as in the paper.
```
$ mpirun -np 8 FreeFem++-mpi diffusion-2d.edp -v 0
$ mpirun -np 8 FreeFem++-mpi diffusion-3d.edp -v 0
$ mpirun -np 8 FreeFem++-mpi elasticity-2d.edp -v 0
$ mpirun -np 8 FreeFem++-mpi elasticity-3d.edp -v 0
```
The option `-v 0` is here to minimize the output generated by FreeFEM, see this [tutorial](http://jolivet.perso.enseeiht.fr/FreeFem-tutorial/#pf1c) for more information.
Some timings are stored in a subfolder as `.txt` files, and it is possible to postprocess solutions using [ParaView](https://www.paraview.org/).

Using the previous commands, one should get four convergence histories per `.edp` script. The first (resp. second) one corresponds to the linear system being solved with an exact *LU* decomposition of the level 2 coarse operator with two (resp. four) MPI processes. The third (resp. fourth) convergence history corresponds to the linear system being solved with the multilevel approach proposed in the paper using two (resp. four) superdomains.
### Customization
Here are the main command line parameters common to all scripts.

* `-global` (default to different values depending on the script), number of discretization points of the global domain
* `-split` (default to `1`), number of local uniform refinement steps after the global domain has been decomposed
* `-base` (default to `2`), sets how the number of superdomains varies: *base*<sup>*i*</sup>, for *i* = 1, ..., 4
* `-no_timing`, if used, nothing is written in the subdirectory `timing/`
* `-output`, if used, a `.pvd` and various `.vtu` are generated in the subdirectory `output/`
* `-Dpetsc=true`, if used, the systems are solved using [PETSc](https://www.mcs.anl.gov/petsc/) instead of HPDDM. In particular, options are provided for [PCHYPRE](https://www.mcs.anl.gov/petsc/petsc-current/docs/manualpages/PC/PCHYPRE.html) and [PCGAMG](https://www.mcs.anl.gov/petsc/petsc-current/docs/manualpages/PC/PCGAMG.html)

## Acknowledgements
* European Commission [Horizon 2020](https://ec.europa.eu/programmes/horizon2020/en) programme, project [NLAFET](http://www.nlafet.eu/), grant number 671633
* HPC resources of [TGCC@CEA](http://www-hpc.cea.fr/index-en.htm) under the allocation A0050607519 made by [GENCI](http://www.genci.fr/en)
